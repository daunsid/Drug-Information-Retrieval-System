{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce53b710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "figcaption {\n",
       "  color: #4e181b;\n",
       "  font-style: italic;\n",
       "  font-size: 16px;\n",
       "  padding: 0px;\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<center \n",
       "\">\n",
       "<img width=\"50%\" src=\"SemanticSearch.png?w=200\">\n",
       "<br/>\n",
       "<a href=\"https://www.kaggle.com/oluwadaunsid\" style=\"color: white;\n",
       "background-color: #2b4b82;\n",
       "border-radius: 25px;\n",
       "padding: 1rem 1.5rem;\n",
       "text-decoration: none;\n",
       "\">@daunsid</a>\n",
       "</center>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "figcaption {\n",
    "  color: #4e181b;\n",
    "  font-style: italic;\n",
    "  font-size: 16px;\n",
    "  padding: 0px;\n",
    "  text-align: center;\n",
    "}\n",
    "</style>\n",
    "<center \n",
    "\">\n",
    "<img width=\"50%\" src=\"SemanticSearch.png?w=200\">\n",
    "<br/>\n",
    "<a href=\"https://www.kaggle.com/oluwadaunsid\" style=\"color: white;\n",
    "background-color: #2b4b82;\n",
    "border-radius: 25px;\n",
    "padding: 1rem 1.5rem;\n",
    "text-decoration: none;\n",
    "\">@daunsid</a>\n",
    "</center>\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cd9e986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data preprocessing libraries\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "import pyarrow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import (\n",
    "    AutoModel, AutoConfig, \n",
    "    AutoTokenizer, logging,\n",
    ")\n",
    "\n",
    "from sentence_transformers import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d939f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"model_name\": \"sentence-transformers/all-MiniLM-L6-v2\",# \"distilbert-base-uncased\",\n",
    "    \"model_name_path\":\"../information_retrieval/weights/model\",\n",
    "    \"device\": 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \"max_length\": 256,\n",
    "}\n",
    "\n",
    "config = AutoConfig.from_pretrained(CONFIG[\"model_name_path\"])\n",
    "#model = AutoModel.from_pretrained(CONFIG[\"model_name\"], config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ca10624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRUGS DATA SHAPE (999, 17)\n"
     ]
    }
   ],
   "source": [
    "# set the path to the dataset to the variable name 'IMG_PATH'\n",
    "DATA_PATH = '../data/raw/Drugs Master List.csv'\n",
    "\n",
    "\n",
    "drugs_df = pd.read_csv(DATA_PATH)\n",
    "print(\"DRUGS DATA SHAPE\", drugs_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69ee8b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['drug_name', 'medical_condition', 'side_effects', 'generic_name',\n",
    "            'drug_classes', 'brand_names', 'activity', 'rx_otc',\n",
    "            'pregnancy_category', 'csa', 'alcohol', 'related_drugs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e5c8d38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drug_name :doxycycline\n",
      "\n",
      " side_effects:(hives, difficult breathing, swelling in your face or throat) or a severe skin reaction (fever, sore throat, burning in your eyes, skin pain, red or purple skin rash that spreads and causes blistering and peeling). Seek medical treatment if you have a serious drug reaction that can affect many parts of your body. Symptoms may include: skin rash, fever, swollen glands, flu-like symptoms, muscle aches, severe weakness, unusual bruising, or yellowing of your skin or eyes. This reaction may occur several weeks after you began using doxycycline. Doxycycline may cause serious side effects. Call your doctor at once if you have: severe stomach pain, diarrhea that is watery or bloody; throat irritation, trouble swallowing; chest pain, irregular heart rhythm, feeling short of breath; little or no urination; low white blood cell counts - fever, chills, swollen glands, body aches, weakness, pale skin, easy bruising or bleeding; severe headaches, ringing in your ears, dizziness, nausea, vision problems, pain behind your eyes; loss of appetite, upper stomach pain (that may spread to your back), tiredness, nausea or vomiting, fast heart rate, dark urine, jaundice (yellowing of the skin or eyes). Common side effects of doxycycline may include: nausea and vomiting; upset stomach; loss of appetite; mild diarrhea; skin rash or itching; darkened skin color; vaginal itching or discharge.\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# \n",
    "\n",
    "sample1 = drugs_df[drugs_df['drug_name']=='doxycycline']\n",
    "print(f\"drug_name :{sample1['drug_name'][0]}\\n\\n \\\n",
    "side_effects:{sample1['side_effects'][0]}\\n\\n \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "961989ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features: 17\n"
     ]
    }
   ],
   "source": [
    "# let see the different features availbale in our data\n",
    "\n",
    "features = {i:col for i, col in enumerate(drugs_df.columns)}\n",
    "print(f\"Number of Features: {len(features)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a196dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_string(series):\n",
    "    sentence = ''\n",
    "    for words in series:\n",
    "        sentence += words+\"\\n\"\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "class PreprocessPipeLine:\n",
    "    \n",
    "    def __call__(self, df):\n",
    "        \"\"\"\n",
    "        drop redundant features not necesaary for retrieval system\n",
    "        `data_df.drop(list of columns to drop, axis=1)`\n",
    "        \"\"\"\n",
    "        df = df.drop(['rating','no_of_reviews',\n",
    "                      'drug_link','medical_condition_url',\n",
    "                      'medical_condition_description'],\n",
    "                     axis=1)\n",
    "        # replace null values with the string unknown\n",
    "        df = df.fillna('unknown')\n",
    "        \n",
    "        df['related_drugs'] = df['related_drugs'].apply(lambda z: self.remove_url_char(z))\n",
    "        \n",
    "        # explode `side_effects` column\n",
    "        df['side_effects'] = df['side_effects'].apply(lambda z: z.split('.'))\n",
    "        df = df.explode('side_effects', ignore_index=True)\n",
    "        \n",
    "        # drop rows with empty side_effects\n",
    "        df['string_length'] = df['side_effects'].apply(lambda z: len(z))\n",
    "        df = df[df['string_length']>0]\n",
    "        df.index = [i for i in range(len(df))]\n",
    "        df = df.drop('string_length', axis=1)\n",
    "        return df\n",
    "    \n",
    "    def remove_url_char(self, feature):\n",
    "        # clean related drugs:\n",
    "            #remove unwanted url links from \n",
    "            #remove characters '|' and spaces\n",
    "        url_cleaner = re.compile(r\":|https://\\S+|www\\.\\S+\")\n",
    "        feature = url_cleaner.sub(r'', feature)\n",
    "        feature = feature.strip().replace(r'  | ', ', ')\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "126185fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrugsInformation(torch.utils.data.Dataset):\n",
    "    def __init__(self, drugs):\n",
    "        #super(DrugsInformation, self).__init__()\n",
    "        self.drugs = drugs\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(self.drugs, pd.DataFrame):\n",
    "            drug = self.drugs.loc[idx]\n",
    "                    #drug_info = self.drugs.loc[idx]\n",
    "        else:\n",
    "            drug = self.drugs[idx]\n",
    "        \n",
    "        drug= to_string(drug)   \n",
    "        return drug\n",
    "    def __len__(self):\n",
    "        return len(self.drugs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32e32017",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = PreprocessPipeLine()\n",
    "\n",
    "drugs_df = preprocess(drugs_df)\n",
    "drugs_data = DrugsInformation(drugs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "74d5281e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data successfully saved in ../data/processed/drug.parquet\n"
     ]
    }
   ],
   "source": [
    "# save the data in parquet format\n",
    "\n",
    "def save_data(data:pd.DataFrame, dataset_file:str):\n",
    "    if not os.path.exists(dataset_file):\n",
    "        data.to_parquet(dataset_file,\n",
    "                        engine='pyarrow',\n",
    "                        index=False)\n",
    "        print(f'data successfully saved in {dataset_file}')\n",
    "save_data(drugs_df, '../data/processed/drug.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "473af945",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_00 = drugs_data[6934]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f5bbb5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histafed\n",
      "Colds & Flu\n",
      " Common side effects of Histafed may include: dizziness, drowsiness; dry mouth, nose, or throat; constipation ; blurred vision; or feeling restless or excited (especially in children)\n",
      "pseudoephedrine and triprolidine\n",
      "Upper respiratory combinations\n",
      "Aphedrid, A-Phedrin, Aprodine, Vi-Sudo\n",
      "0%\n",
      "Rx/OTC\n",
      "C\n",
      "N\n",
      "X\n",
      "unknown\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] histafed colds & flu common side effects of histafed may include : dizziness, drowsiness ; dry mouth, nose, or throat ; constipation ; blurred vision ; or feeling restless or excited ( especially in children ) pseudoephedrine and triprolidine upper respiratory combinations aphedrid, a - phedrin, aprodine, vi - sudo 0 % rx / otc c n x unknown [SEP]'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data_00)\n",
    "ttxts=tokenizer(data_00)\n",
    "tokenizer.decode(ttxts['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88274038",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(drugs_data, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73835a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(texts):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name_path\"])\n",
    "    encoded_texts = tokenizer(\n",
    "        texts,add_special_tokens=True,padding='max_length',\n",
    "        max_length=int(CONFIG[\"max_length\"]),\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "        #return_attention_mask=True\n",
    "    )                          \n",
    "    return encoded_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17df1e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, model_name_path):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_name_path)\n",
    "        self.model = AutoModel.from_pretrained(model_name_path, from_tf=True, config=self.config)\n",
    "\n",
    "    def forward(self, **encoded_inputs):\n",
    "        out = self.model(**encoded_inputs)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6133740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "class MeanPooling(nn.Module) :\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "\n",
    "    def forward(self, model_output, attention_mask) -> torch.tensor :\n",
    "\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask =  torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "def drugs_information_retrieval(texts, embeddings):\n",
    "    inputs = to_string(texts)\n",
    "    inputs = get_tokenizer(texts)\n",
    "    \n",
    "    prediction = get_embeddings(inputs).detach().cpu()\n",
    "    #outputs = F.cosine_similarity(prediction, embeddings)\n",
    "    embeddings = embeddings.detach().cpu()\n",
    "    outputs = util.semantic_search(prediction, embeddings, top_k=1)[0][0]\n",
    "    \n",
    "    return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b3f88a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a12174e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_embeddings(encoded_inputs):\n",
    "    encoded_inputs = {k: v.to(CONFIG[\"device\"]) for k, v in encoded_inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        model = Encoder(CONFIG[\"model_name_path\"]).to(CONFIG[\"device\"])\n",
    "        model_output = model(**encoded_inputs)\n",
    "        \n",
    "    pooler = MeanPooling()\n",
    "    embeddings = pooler(model_output, encoded_inputs['attention_mask'])\n",
    "    embeddings = F.normalize(embeddings)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55549329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "list_embeddings = []\n",
    "for dl in dataloader:\n",
    "    enc_inputs = get_tokenizer(dl)\n",
    "    embedding = get_embeddings(enc_inputs)\n",
    "    \n",
    "    list_embeddings.append(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a321d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = torch.cat(list_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e10b8d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBD_PATH = '../information_retrieval/weights/embd/embeddings.pickle'\n",
    "with open(EMBD_PATH, 'wb') as f: \n",
    "    pickle.dump(embeddings, f)\n",
    "    f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ba01bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8d154e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b18e65f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "PROCESS_DATA_PATH=\"../data/processed/drug.parquet\"\n",
    "def get_DI(idx):\n",
    "    information = DrugsInformation(pd.read_parquet(PROCESS_DATA_PATH))[idx]\n",
    "    return information\n",
    "tx = \"I need information about Histafed\"\n",
    "\n",
    "output = drugs_information_retrieval(tx, embeddings)\n",
    "idx, score = output['corpus_id'], output['score']\n",
    "drug_info = get_DI(idx).split('\\n')\n",
    "results = {field:info\n",
    "           for field, info in zip(fields, drug_info)\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b34e59c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'drug_name': 'Histafed',\n",
       " 'medical_condition': 'Colds & Flu',\n",
       " 'side_effects': ' Common side effects of Histafed may include: dizziness, drowsiness; dry mouth, nose, or throat; constipation ; blurred vision; or feeling restless or excited (especially in children)',\n",
       " 'generic_name': 'pseudoephedrine and triprolidine',\n",
       " 'drug_classes': 'Upper respiratory combinations',\n",
       " 'brand_names': 'Aphedrid, A-Phedrin, Aprodine, Vi-Sudo',\n",
       " 'activity': '0%',\n",
       " 'rx_otc': 'Rx/OTC',\n",
       " 'pregnancy_category': 'C',\n",
       " 'csa': 'N',\n",
       " 'alcohol': 'X',\n",
       " 'related_drugs': 'unknown'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
